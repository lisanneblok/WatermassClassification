{"cells":[{"cell_type":"markdown","metadata":{"id":"9C68QWKso8OX"},"source":["# Gaussian Process Classification step 8\n","\n","### Methods\n","The Gaussian Process Classifier (GPC) is a probabilistic, non-parametric machine learning method. It is particularly well-suited for classification tasks where the decision boundaries are complex and not easily defined by simple linear models. GPC models the data distribution using a Gaussian process, which allows it to capture non-linear relationships in the data.\n","\n","We will assess a few likely kernels and see which performs best on a subset of the data. This is done in the context of computational efficiency, as GPCs can be computationally heavy, especially when being performed in many dimensions.\n","\n","\n","### Workflow\n","- Gaussian Process Classifier:\n","\n","    We will create an instance of the Gaussian Process Classifier with an RBF kernel and train it on the preprocessed data.\n","\n","    We will iterate through the kernels defined in step 7 and see which kernel performs best on our downscaled data. The best performing kernel will then be chosen.\n","\n","- Model Evaluation:\n","\n","    We will evaluate the performance of the GPC model by measuring its accuracy and other relevant metrics.\n","\n","In the next notebook regarding steps, 9 and 10, we will upscale our model to more data using the preferred kernel chosen in this notebook."]},{"cell_type":"markdown","metadata":{"id":"oMj9wFDLwA3d"},"source":["### Install relevant dependencies"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"12JLFtxMDjZ-","executionInfo":{"status":"ok","timestamp":1701769440527,"user_tz":-60,"elapsed":36010,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"}}},"outputs":[],"source":["%%capture\n","!pip install cartopy\n","!pip install gpflow"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"k60kjKzVo8Ob","executionInfo":{"status":"ok","timestamp":1701769487181,"user_tz":-60,"elapsed":2,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"}}},"outputs":[],"source":["import sys\n","import os\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from sklearn.utils import resample\n","sys.path.append(\"/Users/Lisanne/Documents/AI4ER/PhD/GPs\")\n","\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","import warnings\n","\n","# Suppress all warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18947,"status":"ok","timestamp":1701769462595,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"},"user_tz":-60},"id":"kvd05NM6phmq","outputId":"16c785d8-6a47-4c65-801e-c95d45f854e8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CzANm3Uso8Oc","executionInfo":{"status":"ok","timestamp":1701769464291,"user_tz":-60,"elapsed":1700,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"}}},"outputs":[],"source":["# Specify the file path for the pickle file\n","mean_df = pd.read_pickle(\"/content/drive/MyDrive/ai4er/GPs/data/processed/ml_ready/coldwarmclass.pkl\")\n","#mean_df = pd.read_pickle(\"/content/drive/MyDrive/ai4er/GPs/data/processed/ml_ready/means_8.pkl\")"]},{"cell_type":"markdown","metadata":{"id":"QfFN8B-m3Yp-"},"source":["## Data preparation: recap from 1-4"]},{"cell_type":"markdown","source":["### Sample down dataframe\n","ITP and Argos data from 2006, 2007 and 2008 contain 269,671 datapoints.\n","For this interpolation task, the dimensionality D = 4, as we will use latitude, longitude, depth and time. We have N = 269671 collected datapoints.\n","\n","To cut down on computational costs, the dataframe will be sampled down first. By grouping by latitude and longitude, the spatial variation will be maintained."],"metadata":{"id":"FjNmctXdRl2u"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"uzJocOE-rJoL","executionInfo":{"status":"ok","timestamp":1701769473683,"user_tz":-60,"elapsed":9394,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","# Group by latitude and longitude\n","grouped_df = mean_df.groupby(['latitude', 'longitude'])\n","\n","# Define the target number of samples per group (adjust as needed)\n","target_samples = 5\n","\n","# Initialize an empty DataFrame for the balanced samples\n","new_df = pd.DataFrame()\n","\n","# Iterate over groups and randomly subsample to achieve balance\n","for group_name, group_data in grouped_df:\n","    if len(group_data) >= target_samples:\n","        # If the group has enough samples, randomly subsample\n","        sampled_data = group_data.sample(target_samples, replace=False)\n","    else:\n","        # If the group has fewer samples, use all of them\n","        sampled_data = group_data\n","    new_df = pd.concat([new_df, sampled_data])\n","\n","# Reset the index of the balanced DataFrame\n","new_df.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"p-S5IXvfB2te","executionInfo":{"status":"ok","timestamp":1701769473684,"user_tz":-60,"elapsed":11,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"}}},"outputs":[],"source":["merged_df = new_df\n","\n","# Count the occurrences of each class in the \"WaterMass\" column\n","class_counts = merged_df[\"WaterMass\"].value_counts()\n","\n","# Find the count of the third class\n","desired_class_count = class_counts[3]\n","\n","# Create an empty DataFrame to store the balanced data\n","balanced_df = pd.DataFrame()\n","\n","# Iterate over unique classes and balance the samples based on the count of the third class\n","for unique_class in merged_df[\"WaterMass\"].unique():\n","    class_data = merged_df[merged_df[\"WaterMass\"] == unique_class]\n","    class_balanced = resample(class_data, n_samples=desired_class_count, random_state=42)\n","    balanced_df = pd.concat([balanced_df, class_balanced])"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"RGSdj8jJ8NLG","executionInfo":{"status":"ok","timestamp":1701769490541,"user_tz":-60,"elapsed":284,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"}}},"outputs":[],"source":["ref_latitude = 50\n","ref_longitude = -100\n","\n","# Function to calculate angle between two points and the reference point\n","def calculate_angle(lat, lon):\n","  lat_rad, lon_rad = map(math.radians, [lat, lon])\n","  ref_lat_rad, ref_lon_rad = map(math.radians, [ref_latitude, ref_longitude])\n","\n","  delta_lon = lon_rad - ref_lon_rad\n","\n","  y = math.sin(delta_lon) * math.cos(lat_rad)\n","  x = math.cos(ref_lat_rad) * math.sin(lat_rad) - (math.sin(ref_lat_rad) * math.cos(lat_rad) * math.cos(delta_lon))\n","\n","  angle_rad = math.atan2(y, x)\n","  angle_deg = math.degrees(angle_rad)\n","  return angle_deg\n","\n","# Calculate angle for each point in the DataFrame\n","balanced_df['bearing'] = balanced_df.apply(lambda row: calculate_angle(row['latitude'], row['longitude']), axis=1)\n","\n","def haversine(lat1, lon1, lat2, lon2):\n","    R = 6371000  # Earth radius in meters\n","    dlat = np.radians(lat2 - lat1)\n","    dlon = np.radians(lon2 - lon1)\n","    a = np.sin(dlat / 2) * np.sin(dlat / 2) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(dlon / 2) * np.sin(dlon / 2)\n","    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n","    distance = R * c\n","    return distance\n","\n","# Calculate distances from each point to the reference point\n","balanced_df['distance'] = haversine(ref_latitude, ref_longitude, balanced_df['latitude'], balanced_df['longitude'])\n","\n","# Assuming 'balanced_df' already contains a 'month' column\n","balanced_df['seasonal_sin'] = np.sin(2 * np.pi * balanced_df['month'] / 12)"]},{"cell_type":"markdown","metadata":{"id":"VIuFL7VuQFAD"},"source":["## Data preparation\n","Gaussian processes (GPs) are a flexible and non-parametric tool for regression and classification tasks. However, for large datasets, exact GPs can become computationally expensive because their time and space complexity scales cubically with the number of data points."]},{"cell_type":"markdown","metadata":{"id":"afTo0B1vyw9L"},"source":["Labels are encoded in a one-hot fashion, so that if if C = 4 (classes), and y = 2, y is encoded as y = [0,1,0,0]."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"oDs3_Utv9Lh2","executionInfo":{"status":"ok","timestamp":1701769582850,"user_tz":-60,"elapsed":4350,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f5aef43-be44-4102-831c-f92628bc6960"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train Dataset Size: 1106\n","Validation Dataset Size: 249\n","Test Dataset Size: 293\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.cluster import KMeans\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler, LabelEncoder\n","\n","n_clusters_per_region = 80\n","\n","# Use KMeans for clustering within each region\n","kmeans_clusters = KMeans(n_clusters=n_clusters_per_region, n_init=n_clusters_per_region, random_state=7)\n","balanced_df['cluster'] = kmeans_clusters.fit_predict(balanced_df[['latitude', 'longitude']])\n","\n","# Determine which clusters to assign to train, val, and test\n","cluster_indices = balanced_df['cluster'].unique()\n","\n","# Assuming 'cluster' is the column containing cluster labels in balanced_df\n","clusters = balanced_df['cluster'].unique()\n","\n","# Split clusters into training and validation sets\n","train_clusters, split_clusters = train_test_split(clusters, test_size=0.3, random_state=42)\n","val_clusters, test_clusters = train_test_split(split_clusters, test_size=0.4, random_state=42)\n","\n","# Create training and validation datasets\n","train_dataset = balanced_df[balanced_df['cluster'].isin(train_clusters)]\n","val_dataset = balanced_df[balanced_df['cluster'].isin(val_clusters)]\n","test_dataset = balanced_df[balanced_df['cluster'].isin(test_clusters)]\n","\n","# Verify the size of each dataset\n","print(\"Train Dataset Size:\", len(train_dataset))\n","print(\"Validation Dataset Size:\", len(val_dataset))\n","print(\"Test Dataset Size:\", len(test_dataset))"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"g-vaKFydARvN","executionInfo":{"status":"ok","timestamp":1701769585845,"user_tz":-60,"elapsed":241,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"}}},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, LabelEncoder\n","import tensorflow as tf\n","\n","# List of feature names\n","feature_names = ['distance', 'bearing', 'depth', 'seasonal_sin']\n","\n","# Extract features and target variable from train_dataset\n","X_train_dataset = train_dataset[feature_names]\n","y_train_dataset = train_dataset['WaterMass']\n","\n","# Extract features and target variable from val_dataset\n","X_val_dataset = val_dataset[feature_names]\n","y_val_dataset = val_dataset['WaterMass']\n","\n","# Extract features and target variable from test_dataset\n","X_test_dataset = test_dataset[feature_names]\n","y_test_dataset = test_dataset['WaterMass']\n","\n","# Encode categorical labels to numerical values using LabelEncoder\n","label_encoder = LabelEncoder()\n","y_train_encoded_dataset = label_encoder.fit_transform(y_train_dataset).reshape(-1, 1)\n","y_val_encoded_dataset = label_encoder.transform(y_val_dataset).reshape(-1, 1)\n","y_test_encoded_dataset = label_encoder.transform(y_test_dataset).reshape(-1, 1)\n","\n","# Reshape without creating an additional dimension\n","y_train_encoded_dataset = y_train_encoded_dataset.reshape(-1)\n","y_val_encoded_dataset = y_val_encoded_dataset.reshape(-1)\n","y_test_encoded_dataset = y_test_encoded_dataset.reshape(-1)\n","\n","# Standardize the features using StandardScaler\n","scaler = StandardScaler()\n","X_train_scaled_dataset = scaler.fit_transform(X_train_dataset)\n","X_val_scaled_dataset = scaler.transform(X_val_dataset)\n","X_test_scaled_dataset = scaler.transform(X_test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"mSwf8HI7ZccH"},"source":["# GaussianProcessClassifier\n","\n","This function, `evaluate_kernel`, assesses the performance of a given Gaussian process kernel using the `GaussianProcessClassifier` from scikit-learn. The function takes as input the training and validation features (`X_train` and `X_val`), corresponding labels (`y_train` and `y_val`), and the kernel to be evaluated.\n","\n","### Parameters:\n","\n","- `X_train`: Training features.\n","- `y_train`: Training labels.\n","- `X_val`: Validation features.\n","- `y_val`: Validation labels.\n","- `kernel`: The Gaussian process kernel to be evaluated.\n","\n","### Returns:\n","\n","- `accuracy`: Accuracy score of the model on the validation set.\n","\n","### Functionality:\n","\n","1. **GaussianProcessClassifier Initialization:**\n","    - Creates a `GaussianProcessClassifier` instance with the specified kernel and a set number of restarts for optimization (`n_restarts_optimizer`).\n","\n","2. **Model Training:**\n","    - Fits the classifier to the training data (`X_train`, `y_train`).\n","\n","3. **Prediction:**\n","    - Uses the trained model to predict labels for the validation set (`X_val`).\n","\n","4. **Accuracy Calculation:**\n","    - Calculates the accuracy score by comparing predicted labels with true labels on the validation set.\n","\n","5. **Prints Results:**\n","    - Displays the accuracy of the model with the current kernel.\n","\n","6. **Confusion Matrix Visualization:**\n","    - Generates and visualizes the confusion matrix for the predictions on the validation set.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"d9f2uiOKbdhh","executionInfo":{"status":"ok","timestamp":1701769598091,"user_tz":-60,"elapsed":199,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"}}},"outputs":[],"source":["from sklearn.gaussian_process.kernels import RBF, WhiteKernel, Matern, ExpSineSquared, ConstantKernel as C\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","def evaluate_kernel(X_train, y_train, X_val, y_val, kernel):\n","    \"\"\"\n","    Evaluate a single kernel using GaussianProcessClassifier.\n","\n","    Parameters:\n","    - X_train: Training features\n","    - y_train: Training labels\n","    - X_val: Validation features\n","    - y_val: Validation labels\n","    - kernel: Kernel to evaluate\n","\n","    Returns:\n","    - accuracy: Accuracy for the given kernel\n","    \"\"\"\n","\n","    # Create GaussianProcessClassifier with the current kernel\n","    model_sklearn = GaussianProcessClassifier(kernel=kernel, n_restarts_optimizer=10)\n","\n","    # Fit the model to the training data\n","    model_sklearn.fit(X_train, y_train)\n","\n","    # Make predictions on the validation set\n","    predicted_classes = model_sklearn.predict(X_val)\n","\n","    # Calculate accuracy on the validation set\n","    accuracy = accuracy_score(y_val, predicted_classes)\n","\n","    # Print the accuracy for the current kernel\n","    print(f\"Validation Accuracy ({str(kernel)}): {accuracy * 100:.2f}%\")\n","\n","    # Plot confusion matrix for the current kernel\n","    conf_matrix = confusion_matrix(y_val, predicted_classes)\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'Confusion Matrix ({str(kernel)})')\n","    plt.show()\n","\n","    return accuracy, model_sklearn, predicted_classes"]},{"cell_type":"markdown","metadata":{"id":"XJEypa9IZ4Vx"},"source":["# Define kernels to evaluate on\n","\n","## RBF Kernel for Classification\n","\n","The Radial Basis Function (RBF) kernel, also known as the Gaussian kernel, is commonly used in Gaussian process-based classification tasks. Here's an explanation of why the RBF kernel is often preferred for classification:\n","\n","##Smooth and Flexible Decision Boundaries:\n","\n","- **Smoothness:** The RBF kernel generates smooth and continuous decision boundaries, making it suitable for capturing complex relationships between features. This smoothness is particularly advantageous in classification problems where decision boundaries might be intricate or nonlinear.\n","\n","- **Flexibility:** The RBF kernel can model complex decision regions, allowing the classifier to adapt well to varying patterns in the data. This flexibility is crucial when dealing with datasets that exhibit intricate class structures.\n","\n","##Versatility in Capturing Patterns:\n","\n","- **Capturing Local Patterns:** The RBF kernel is effective at capturing local patterns and variations in the data. This is beneficial for classifying instances that may have different local characteristics within the feature space.\n","\n","- **Implicit Feature Mapping:** The RBF kernel implicitly performs a non-linear feature mapping, allowing it to handle high-dimensional data and discover intricate relationships between features without explicitly expanding the feature space.\n","\n","### Hyperparameter Control:\n","\n","- **Length Scale Parameter:** The length scale parameter in the RBF kernel controls the smoothness and width of the decision boundaries. Adjusting this parameter provides a way to regulate the kernel's sensitivity to variations in the data, allowing fine-tuning of the model's generalization capacity.\n","\n","### General Applicability:\n","\n","- **Commonly Used:** The RBF kernel is a widely adopted choice in practice due to its effectiveness in capturing complex relationships and its suitability for a broad range of classification tasks.\n","\n","In the provided code snippet, the RBF kernel is created with a specific length scale of 1.0. The choice of this length scale can be adjusted based on the characteristics of the dataset and the desired smoothness of the decision boundaries.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"x2pqJhzjZ36J","executionInfo":{"status":"ok","timestamp":1701769601540,"user_tz":-60,"elapsed":242,"user":{"displayName":"Lisanne Blok","userId":"02031248107663279774"}}},"outputs":[],"source":["# Create an RBF kernel with the specified length scale\n","RBF_kernel = 1 * RBF(length_scale = 1.0)"]},{"cell_type":"code","source":[],"metadata":{"id":"nyPJtDcOh2n9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bUZNQHCYKg4L"},"outputs":[],"source":["accuracy_rbf, model_rbf, predicted_rbf = evaluate_kernel(X_train_scaled_dataset, y_train_encoded_dataset,\n","                               X_val_scaled_dataset, y_val_encoded_dataset,\n","                               1.0 * RBF(length_scale=1.0))"]},{"cell_type":"code","source":["# Get the weights of the model\n","model_weights = model_rbf.kernel_\n","# Print or use the obtained information\n","print(f\"Optimized Kernel Parameters: {model_weights}\")"],"metadata":{"id":"SynPDt9pL9YI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Matern Kernel for Classification\n","\n","The Matern kernel is another commonly used kernel in Gaussian process-based classification, offering certain advantages and characteristics. Let's explore why the 1.0 * Matern kernel might be chosen for classification tasks:\n","\n","### Robustness and Flexibility:\n","\n","- **Adaptive Smoothing:** The Matern kernel is a versatile choice that provides adaptive smoothing of decision boundaries. It offers a balance between the RBF kernel (infinitely differentiable) and the Laplace kernel (not differentiable), making it a robust option for diverse datasets.\n","\n","- **Flexibility:** The Matern kernel is characterized by a parameter, often denoted as `nu`, which controls the smoothness of the decision boundaries. A higher value of `nu` results in a smoother function, while a lower value introduces more flexibility, allowing the kernel to capture intricate patterns in the data.\n","\n","### Handles Noisy Data:\n","\n","- **Noise Robustness:** The Matern kernel is known for its robustness to noisy data. In scenarios where the classification task involves data with inherent noise or uncertainties, the Matern kernel can provide stable and reliable predictions.\n","\n","### Distance Metric Flexibility:\n","\n","- **Distance Function Control:** The Matern kernel allows flexibility in choosing the distance metric for measuring dissimilarity between data points. This adaptability can be advantageous when dealing with datasets where different features may have distinct scales or importance.\n","\n","### Hyperparameter Adjustments:\n","\n","- **Length Scale Parameter:** Similar to the RBF kernel, the Matern kernel includes a length scale parameter (`length_scale`). Adjusting this parameter allows fine-tuning of the model's sensitivity to variations in the data, impacting the width and smoothness of decision boundaries.\n","\n","### Effective for Spatially Varying Patterns:\n","\n","- **Spatially Varying Patterns:** The Matern kernel is well-suited for capturing spatially varying patterns in the data. This is valuable in classification tasks where the relationships between features exhibit varying degrees of smoothness in different regions of the feature space.\n","\n","Here, the 1.0 * Matern kernel is created with a specific length scale of 1.0. This length scale can be adjusted based on the characteristics of the dataset and the desired trade-off between smoothness and flexibility in the decision boundaries.\n"],"metadata":{"id":"VJ6FDXznNFBv"}},{"cell_type":"code","source":["matern_kernel = 1.0 * Matern(length_scale=1.0)\n","\n","accuracy_matern = evaluate_kernel(X_train_scaled_dataset,\n","                                  y_train_encoded_dataset,\n","                                  X_val_scaled_dataset,\n","                                  y_val_encoded_dataset, matern_kernel)"],"metadata":{"id":"Kr2Ec24RnlzI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6QbFdph9Z8eY"},"source":["## RBF Kernel with Different Length Scales\n","\n","In the context of Gaussian process-based classification, the Radial Basis Function (RBF) kernel is often customized by varying its length scale for each input feature. This allows for an adaptive and feature-specific modeling of the relationships within the data. Let's explore the significance of using different length scales for the features `r`, `theta`, `depth`, and `seasonal_sin`:\n","\n","### Feature-Specific Sensitivity:\n","\n","- **`distance` (Distance from reference point):** The length scale for `distance` determines the range over which data points in radial distance influence each other. A larger length scale suggests that points farther apart contribute to the correlation, capturing patterns that extend over longer distances.\n","\n","- **`theta` (Angular Position):** A smaller length scale for `theta` might be chosen to capture fine-grained patterns associated with small changes in angular position. It allows the kernel to be more sensitive to variations in the angular feature.\n","\n","- **`depth` (Depth):** A length scale of 1.0 for `depth` indicates that data points within a depth range of 1.0 units influence each other. Adjusting the length scale can control how sensitive the kernel is to variations in depth.\n","\n","- **`seasonal_sin` (Seasonal Sine Component):** A smaller length scale for `seasonal_sin` may be suitable for capturing rapid variations in the seasonal sine component. This allows the kernel to adapt to short-term fluctuations in the seasonal pattern.\n","\n","### Trade-off between Smoothness and Sensitivity:\n","\n","- **1.0:** Choosing a length scale of 1.0 generally balances the trade-off between smoothness and sensitivity. It provides a moderate level of sensitivity to variations in each feature without overly emphasizing local fluctuations.\n","\n","- **0.1:** A smaller length scale of 0.1 increases sensitivity, making the kernel more responsive to local variations. This might be desirable when there are rapid changes or fine details in the data that need to be captured.\n","\n","### Adaptability to Feature Characteristics:\n","\n","- **Adaptation:** Adjusting the length scales allows the RBF kernel to adapt to the inherent characteristics of each feature. It provides a means to tailor the model to different scales and patterns present in the data.\n","\n","In the provided code snippet, an RBF kernel is created with different length scales specified for `r`, `theta`, `depth`, and `seasonal_sin`. This customization enables the kernel to effectively capture diverse patterns and relationships within the multidimensional feature space.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHsFMi8hZ585"},"outputs":[],"source":["# Create an RBF kernel with the different length scales\n","length_scale = [1.0, 0.1, 1, 0.1]\n","\n","lengthscale_kernel = RBF(length_scale=length_scale)\n","\n","evaluate_kernel(X_train_scaled_dataset, y_train_encoded_dataset, X_val_scaled_dataset, y_val_encoded_dataset,\n","                lengthscale_kernel)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1UeiFh2qd8t9KUA3bF-GNcX1apK5yYLux","timestamp":1700646669839},{"file_id":"1e8o-MICVXcXQkp4t72t0CAg5WqrQql0u","timestamp":1699892699148},{"file_id":"1ce1qvFJTRETPiYcSYxOA52Xex8TpTDUz","timestamp":1699829539214},{"file_id":"1y5pbJ_lLk1ngfcqzPaHEcbbt9fapRHiH","timestamp":1699616673377}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":0}